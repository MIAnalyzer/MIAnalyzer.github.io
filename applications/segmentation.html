<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Object Detection" href="objectdetection.html" /><link rel="prev" title="Classification" href="classification.html" />

    <!-- Generated with Sphinx 7.0.1 and Furo 2023.05.20 -->
        <title>Segmentation - MIA documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=e6660623a769aa55fea372102b9bf3151b292993" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">MIA  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">MIA  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 has-children"><a class="reference internal" href="../introduction/index.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Introduction</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../introduction/intro.html">MIA: Microscopic Image Analyzer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/installation.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/cite.html">How to cite MIA?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction/acknowledge.html">Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gettingstarted/index.html">Getting Started</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Getting Started</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../gettingstarted/quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gettingstarted/ui.html">The User Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gettingstarted/settings.html">Settings</a></li>
</ul>
</li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Applications</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Applications</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="classification.html">Classification</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="objectdetection.html">Object Detection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../training/index.html">Training</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Training</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../training/nntraining.html">Neural Network Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/trainsettings.html">Training Settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/augmentation.html">Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/trainplot.html">Train Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../prediction/index.html">Prediction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../postprocessing/index.html">Post Processing</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Post Processing</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../postprocessing/postprocessing.html">Postprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../postprocessing/tracking.html">Tracking</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../results/index.html">Results</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="segmentation">
<h1>Segmentation<a class="headerlink" href="#segmentation" title="Permalink to this heading">#</a></h1>
<p>Semantic image segmentation is the process of pixel-by-pixel classification that results in segments that correspond to the same object. Segmentation is typically used to identify and locate objects in an image and to determine their shape and size.</p>
<figure class="align-center" id="id26">
<a class="shadow-image reference internal image-reference" href="../_images/segmentation.png"><img alt="../_images/segmentation.png" class="shadow-image" src="../_images/segmentation.png" style="width: 230px;" /></a>
<figcaption>
<p><span class="caption-text">Segmented image with synaptic spines shown in green and their parent dendrite in red</span><a class="headerlink" href="#id26" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="labelling">
<h2>Labelling<a class="headerlink" href="#labelling" title="Permalink to this heading">#</a></h2>
<p>To train a neural network for semantic segmentation it is necessary to create images that are labelled accordingly.</p>
<figure class="align-center" id="id27">
<img alt="../_images/segmentation_tools.png" class="shadow-image" src="../_images/segmentation_tools.png" />
<figcaption>
<p><span class="caption-text">Segmentation tools</span><a class="headerlink" href="#id27" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Different tools and functions in MIA exist to label images for semantic segmentation.</p>
<p>To label an image select a tool and draw inside the image to label image objects.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is no save option as all changes during labelling are saved immediately. Use <a class="inline-icon reference internal" href="../_images/reset.png"><img alt="reset" class="inline-icon" src="../_images/reset.png" style="width: 16px; height: 16px;" /></a> or <kbd class="kbd docutils literal notranslate">ctrl</kbd> + <kbd class="kbd docutils literal notranslate">z</kbd> to undo the last labelling action.</p>
<p>All labels are saved in a subfolder inside the currently active folder and have the same file name as the currently selected image, but with <code class="code docutils literal notranslate"><span class="pre">.npz</span></code> as file extension.</p>
</div>
<section id="tools">
<h3>Tools<a class="headerlink" href="#tools" title="Permalink to this heading">#</a></h3>
<p>By selecting the <a class="inline-icon reference internal" href="../_images/drag.png"><img alt="drag" class="inline-icon" src="../_images/drag.png" style="width: 16px; height: 16px;" /></a> tool or pressing <kbd class="kbd docutils literal notranslate">F1</kbd> you can always switch to the drag tool to zoom and change the field of view. Press the <kbd class="kbd docutils literal notranslate">Spacebar</kbd> to toggle between drag tool and the last selected tool.</p>
<p>Use the <a class="inline-icon reference internal" href="../_images/draw.png"><img alt="draw" class="inline-icon" src="../_images/draw.png" style="width: 16px; height: 16px;" /></a> tool (or press <kbd class="kbd docutils literal notranslate">F2</kbd>) to label objects with freehand labelling. While pressing the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> you can label target objects in the image.</p>
<p>With the <a class="inline-icon reference internal" href="../_images/poly.png"><img alt="poly" class="inline-icon" src="../_images/poly.png" style="width: 16px; height: 16px;" /></a> tool (or press <kbd class="kbd docutils literal notranslate">F3</kbd>) to label objects with polygones. Press the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> to add add polygone point. When pressing the <kbd class="kbd docutils literal notranslate">right_mouse</kbd>, the first and the last set polygone points are connected resulting in the finished contour.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><a class="inline-icon reference internal" href="../_images/draw.png"><img alt="draw" class="inline-icon" src="../_images/draw.png" style="width: 16px; height: 16px;" /></a> tool and <a class="inline-icon reference internal" href="../_images/poly.png"><img alt="poly" class="inline-icon" src="../_images/poly.png" style="width: 16px; height: 16px;" /></a> tool have different options, press the <kbd class="kbd docutils literal notranslate">right_mouse</kbd> to toggle between them:</p>
<ul class="simple">
<li><p><strong>Add</strong> contours are added to the image from the active class. If contours overlap and are from the same class they are combined to a single contour.</p></li>
<li><p><strong>Delete</strong> removes the created contour from existing contours.</p></li>
<li><p><strong>Slice</strong> creates a line that is removed from existing contours, can be used to split contours.</p></li>
</ul>
</div>
<p>The <a class="inline-icon reference internal" href="../_images/assign.png"><img alt="assign" class="inline-icon" src="../_images/assign.png" style="width: 16px; height: 16px;" /></a> tool (or press <kbd class="kbd docutils literal notranslate">F4</kbd>) can be used to asssign a new class to an existing contour. Select the target class and press the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> inside an existing contour.</p>
<p>The <a class="inline-icon reference internal" href="../_images/expand.png"><img alt="expand" class="inline-icon" src="../_images/expand.png" style="width: 16px; height: 16px;" /></a> tool (or press <kbd class="kbd docutils literal notranslate">F5</kbd>) can be used to correct existing contours. Press and hold the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> to correct existing contours.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><a class="inline-icon reference internal" href="../_images/expand.png"><img alt="expand" class="inline-icon" src="../_images/expand.png" style="width: 16px; height: 16px;" /></a> tool has different options, press the <kbd class="kbd docutils literal notranslate">right_mouse</kbd> to toggle between them and use the <kbd class="kbd docutils literal notranslate">mouse_wheel</kbd> to change the size slider:</p>
<ul class="simple">
<li><p><strong>Expand</strong> allows you to enlarge an existing contour or to create a new contour.</p></li>
<li><p><strong>Erase</strong> removes parts from existing contours.</p></li>
<li><p><strong>Size</strong> slider changes the size of the tool.</p></li>
</ul>
</div>
<p>The <a class="inline-icon reference internal" href="../_images/delete.png"><img alt="delete" class="inline-icon" src="../_images/delete.png" style="width: 16px; height: 16px;" /></a> tool (or press <kbd class="kbd docutils literal notranslate">F6</kbd>) can be used to remove contours. Press the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> inside a contour to remove that contour.</p>
<p>By checking <strong>Inner contours</strong>, contours may have holes. When unchecking <strong>Internal contours</strong> all holes inside of contours are removed, immediately.</p>
</section>
<section id="automated-and-semi-automated-labelling">
<h3>Automated and Semi-Automated Labelling<a class="headerlink" href="#automated-and-semi-automated-labelling" title="Permalink to this heading">#</a></h3>
<p>Different automated and semi-automated segmentation option are implemented in MIA to speed-up image labelling.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All automated methods only affect the current field of view and ignoring everything outside. For multiple complex objects in a larger image, it can be helpful to focus the field of view on each object individually.</p>
</div>
<section id="smart-mode">
<h4>Smart Mode<a class="headerlink" href="#smart-mode" title="Permalink to this heading">#</a></h4>
<p>The <strong>Smart Mode</strong> enables an iterative, interactive segmentation mode based on grabcut <a class="footnote-reference brackets" href="#gc" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.
Perform smartmode segmentation as follows:</p>
<ol class="arabic simple">
<li><p>Select a tool (<a class="inline-icon reference internal" href="../_images/draw.png"><img alt="draw" class="inline-icon" src="../_images/draw.png" style="width: 16px; height: 16px;" /></a>, <a class="inline-icon reference internal" href="../_images/poly.png"><img alt="poly" class="inline-icon" src="../_images/poly.png" style="width: 16px; height: 16px;" /></a>, <a class="inline-icon reference internal" href="../_images/expand.png"><img alt="expand" class="inline-icon" src="../_images/expand.png" style="width: 16px; height: 16px;" /></a>) to roughly label a part of the target structures</p></li>
<li><p>If parts are labelled but should not be labelled remove some of those parts that should be unlabelled</p></li>
<li><p>If parts are not labelled but should be labelled add some of those parts that should be labelled</p></li>
<li><p>Repeat steps 2 and 3</p></li>
<li><p>Select another class and repeat steps 1-4 to label objects of a different class</p></li>
</ol>
</section>
<section id="auto-segmentation">
<h4>Auto-Segmentation<a class="headerlink" href="#auto-segmentation" title="Permalink to this heading">#</a></h4>
<p>The <a class="inline-icon reference internal" href="../_images/magicwand.png"><img alt="magicwand" class="inline-icon" src="../_images/magicwand.png" style="width: 16px; height: 16px;" /></a> <strong>Auto Seg</strong> function performs deep learning based edge detection based on Holistically-nested edge detection <a class="footnote-reference brackets" href="#hed" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>Auto-Segmentation is based on edge detection, to get proper labels you might uncheck the <strong>Inner Contours</strong> property.</p></li>
<li><p>Auto-Segmentation works best for isolated object with a clear boundary.</p></li>
<li><p>The auto-detected edges might also be used as a prior to further labelling</p></li>
</ul>
</div>
</section>
<section id="dextr">
<h4>DEXTR<a class="headerlink" href="#dextr" title="Permalink to this heading">#</a></h4>
<p><a class="inline-icon reference internal" href="../_images/dextr.png"><img alt="dextr" class="inline-icon" src="../_images/dextr.png" style="width: 16px; height: 16px;" /></a> <strong>DEXTR</strong> is deep learning based method that uses the objects extreme points to label the target object <a class="footnote-reference brackets" href="#id25" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<p>Press the <kbd class="kbd docutils literal notranslate">left_mouse</kbd> on the 4 extreme points (top, left, right, bottom) of the target object and that object is segmented and assigned the active class.
The order of the points is irrelevant.</p>
</section>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h2>
<p>For details about neural network training see <a class="reference internal" href="../training/index.html"><span class="doc">Training</span></a>. However, some settings, the network architectures, the loss functions, metrics are specific for segmentation.</p>
<section id="segmentation-settings">
<h3>Segmentation Settings<a class="headerlink" href="#segmentation-settings" title="Permalink to this heading">#</a></h3>
<p>To open the segmenation settings, press:
<a class="inline-icon reference internal" href="../_images/train.png"><img alt="train" class="inline-icon" src="../_images/train.png" style="width: 16px; height: 16px;" /></a> <em>Train Model</em> → <a class="inline-icon reference internal" href="../_images/settings.png"><img alt="settings" class="inline-icon" src="../_images/settings.png" style="width: 16px; height: 16px;" /></a> <em>Segmentation</em>.</p>
<figure class="align-center" id="id28">
<img alt="../_images/segmentation_settings.png" class="shadow-image" src="../_images/segmentation_settings.png" />
<figcaption>
<p><span class="caption-text">Segmentation training settings</span><a class="headerlink" href="#id28" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Checking <strong>Separate Contours</strong> will weight pixels in the proximity of 2 contours higher, increasing the likelyhood that contours that are close to each other will be separated <a class="footnote-reference brackets" href="#unet" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>. Recommended for a dataset with an high density of objects of the same class or with clustered objects.
This option might decrease training speed as the pixels weights need to be calculated for each image.</p>
<p>Checking <strong>Prefer labelled parts</strong> will <strong>Discard up to x background tiles</strong> in which are less than <strong>Minimum required labelled pixels</strong>, which can be set in the corresponding fields. For largely unbalanced datasets with a lot of background and fewer objects this option is recommended.
For unbalanced datasets see also class weighting in <a class="reference internal" href="../training/trainsettings.html"><span class="doc">Training Settings</span></a>.</p>
</section>
<section id="neural-network-architectures">
<span id="segarchitectures"></span><h3>Neural Network architectures<a class="headerlink" href="#neural-network-architectures" title="Permalink to this heading">#</a></h3>
<p>Most neural networks for semantic segmentation are based on a Fully Convolutional Network architecture ommiting fully connected layers <a class="footnote-reference brackets" href="#fcn" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.
During the training process patches of the input images are generated and fed into the network and compared to the corresponding label patch. The size of those patches can be set in <a class="reference internal" href="../training/augmentation.html"><span class="doc">Augmentation</span></a>.</p>
<p>The following Network architectures are currently supported, please refer to the papers for details:</p>
<ul class="simple">
<li><p>U-Net <a class="footnote-reference brackets" href="#unet" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></li>
<li><p>Feature Pyramid Network (FPN) <a class="footnote-reference brackets" href="#fpn" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p></li>
<li><p>LinkNet <a class="footnote-reference brackets" href="#lnkn" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a></p></li>
<li><p>Pyramid Scene Parsing Network (PSPNet) <a class="footnote-reference brackets" href="#psp" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p></li>
<li><p>DeepLabv3+ <a class="footnote-reference brackets" href="#dlv3" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></li>
</ul>
<p>The following backbones are currently supported, untrained or with pretrained weights pre-trained on imagenet dataset <a class="footnote-reference brackets" href="#imn" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>Model</strong></p></td>
<td><p><strong>Options</strong></p></td>
<td><p><strong>Ref.</strong></p></td>
</tr>
<tr class="row-even"><td><p>DenseNet</p></td>
<td><p>densenet121, densenet169, densenet201</p></td>
<td><p><a class="footnote-reference brackets" href="#dns" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNet</p></td>
<td><p>efficientnetb0, efficientnetb1, efficientnetb2, efficientnetb3, efficientnetb4, efficientnetb5, efficientnetb6, efficientnetb7</p></td>
<td><p><a class="footnote-reference brackets" href="#eff" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>Inception</p></td>
<td><p>inceptionv3, inceptionresnetv2</p></td>
<td><p><a class="footnote-reference brackets" href="#inc" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>MobileNet</p></td>
<td><p>mobilenet, mobilenetv2</p></td>
<td><p><a class="footnote-reference brackets" href="#mob" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>ResNet</p></td>
<td><p>resnet18, resnet34, resnet50, resnet101, resnet152</p></td>
<td><p><a class="footnote-reference brackets" href="#res" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>ResNeXt</p></td>
<td><p>resnext50, resnext101</p></td>
<td><p><a class="footnote-reference brackets" href="#rsx" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>SE-ResNet</p></td>
<td><p>seresnet18, seresnet34, seresnet50, seresnet101, seresnet152</p></td>
<td><p><a class="footnote-reference brackets" href="#sen" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>SE-ResNeXt</p></td>
<td><p>seresnext50, seresnext101</p></td>
<td><p><a class="footnote-reference brackets" href="#sen" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>SENet</p></td>
<td><p>senet154</p></td>
<td><p><a class="footnote-reference brackets" href="#sen" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>VGG</p></td>
<td><p>vgg16, vgg19</p></td>
<td><p><a class="footnote-reference brackets" href="#vgg" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td><p>Xception</p></td>
<td><p>xception</p></td>
<td><p><a class="footnote-reference brackets" href="#xcp" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Not all model backbones are available for all model architectures.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul class="simple">
<li><p>The U-Net is a very popular choice for segmentation, it might be a good starting point as network architecture.</p></li>
<li><p>Generally the numbers behind the backbone architecture gives either the number of convolutional layers (e.g. resnet18) or the model version (e.g. inceptionv3).</p></li>
<li><p>When you have limited computing recources use a small network architecture or a network optimized for efficiency (e.g. mobilenetv2).</p></li>
<li><p>From the supported network-backbones the senet154 shows the highest performance on imagenet classification and the slowest processing time.</p></li>
<li><p>From the supported network-backbones the mobilenet has the fastest processing time and fewest parameters.</p></li>
</ul>
</div>
</section>
<section id="losses-and-metrics">
<h3>Losses and Metrics<a class="headerlink" href="#losses-and-metrics" title="Permalink to this heading">#</a></h3>
<p>For semantic segmentation several objective function have been tested for neural network optimization and directly impact the model training.
Metrics are used to measure the performance of the trained model, but are independent of the optimization and the training process.
The loss and metric functions can be set in  <a class="inline-icon reference internal" href="../_images/train.png"><img alt="train" class="inline-icon" src="../_images/train.png" style="width: 16px; height: 16px;" /></a> <em>Train Model</em> → <a class="inline-icon reference internal" href="../_images/settings.png"><img alt="settings" class="inline-icon" src="../_images/settings.png" style="width: 16px; height: 16px;" /></a> <em>Settings</em>.</p>
<section id="cross-entropy">
<h4>Cross Entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this heading">#</a></h4>
<p>The cross entropy loss is a widely used objective function used for classification and segmentation (which is per pixel classification). It is defined as:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{CE} = -\sum_{i=1}^{n}{p_i log(q_i)},\]</div>
</div>
<p>with <span class="math notranslate nohighlight">\(p_i\)</span> the true label and <span class="math notranslate nohighlight">\(q_i\)</span> the model prediction for the <span class="math notranslate nohighlight">\(i_{th}\)</span> class.</p>
</section>
<section id="focal-loss">
<h4>Focal Loss<a class="headerlink" href="#focal-loss" title="Permalink to this heading">#</a></h4>
<p>The focal loss is an extension of the cross entropy, which improves performance for unbalanced datasets <a class="footnote-reference brackets" href="#fl" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a>. It is defined as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{FL} = -\sum_{i=1}^{n}{(1-q_i)^\gamma p_i log(q_i)},\]</div>
</div>
<p>with <span class="math notranslate nohighlight">\(\gamma\)</span> as the focussing parameter. Default is set <span class="math notranslate nohighlight">\(\gamma = 2\)</span>.</p>
</section>
<section id="kullback-leibler-divergence">
<h4>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this heading">#</a></h4>
<p>The Kullback-Leibler Divergence, sometimes referred as relative entropy, is defined as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{KL} = -\sum_{i=1}^{n}{p_i (log(p_i)-log(q_i))}.\]</div>
</div>
</section>
<section id="dice-loss">
<h4>Dice Loss<a class="headerlink" href="#dice-loss" title="Permalink to this heading">#</a></h4>
<p>The dice loss, which can be used for segmentation of highly imbalanced data <a class="footnote-reference brackets" href="#vnet" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a> and is defined as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{Dice} = -\frac{2 \sum_{i=1}^{n}{p_i q_i}}{\sum_{i=1}^{n}{p_i} + \sum_{i=1}^{n}{q_i}}.\]</div>
</div>
</section>
<section id="intersection-over-union">
<h4>Intersection over Union<a class="headerlink" href="#intersection-over-union" title="Permalink to this heading">#</a></h4>
<p>The intersection over union is very similar to the dice coefficient and a measure for the overlap of the prediction and the ground truth. It is a widely used measure for segmentation model performance and is defined as follows:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{iou} = \frac{\sum_{i=1}^{n}{p_i q_i}}{\sum_{i=1}^{n}{p_i} + \sum_{i=1}^{n}{q_i} - \sum_{i=1}^{n}{p_i q_i}}.\]</div>
</div>
</section>
<section id="pixel-accuracy">
<h4>Pixel Accuracy<a class="headerlink" href="#pixel-accuracy" title="Permalink to this heading">#</a></h4>
<p>The pixel accuracy measures all pixels that are classified correctly:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[L_{acc} = \frac{t_p + t_n}{t_p + t_n + f_p + f_n},\]</div>
</div>
<p>with <span class="math notranslate nohighlight">\(t_p\)</span> the true positives (<span class="math notranslate nohighlight">\(p_i=1\)</span> and <span class="math notranslate nohighlight">\(q_i=1\)</span>), <span class="math notranslate nohighlight">\(t_n\)</span> the true negatives (<span class="math notranslate nohighlight">\(p_i=0\)</span> and <span class="math notranslate nohighlight">\(q_i=0\)</span>), <span class="math notranslate nohighlight">\(f_p\)</span> the false positives (<span class="math notranslate nohighlight">\(p_i=0\)</span> and <span class="math notranslate nohighlight">\(q_i=1\)</span>) and <span class="math notranslate nohighlight">\(f_n\)</span> the false negatives (<span class="math notranslate nohighlight">\(p_i=1\)</span> and <span class="math notranslate nohighlight">\(q_i=0\)</span>).
The pixel accuracy is a misleading measure for imbalanced data.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>A good starting point for choosing a <strong>loss function</strong> is usually the cross entropy loss. When you have imbalanced data, you can switch to focal loss. Dice loss should be used for special cases only, as gradients (and with that the general training) are more unstable.
As <strong>metric function</strong> the intersection over union is for most cases a valid choice.</p>
</div>
</section>
</section>
</section>
<section id="postprocessing">
<h2>Postprocessing<a class="headerlink" href="#postprocessing" title="Permalink to this heading">#</a></h2>
<p>To open the postprocessing window press <a class="inline-icon reference internal" href="../_images/postprocessing.png"><img alt="postprocessing" class="inline-icon" src="../_images/postprocessing.png" style="width: 16px; height: 16px;" /></a> <em>Postprocessing</em>.</p>
<figure class="align-center" id="id29">
<img alt="../_images/segmentation_pp.png" class="shadow-image" src="../_images/segmentation_pp.png" />
<figcaption>
<p><span class="caption-text">Postprocessing options for segmentation</span><a class="headerlink" href="#id29" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>See <a class="reference internal" href="../postprocessing/tracking.html"><span class="doc">Tracking</span></a> for a description of the tracking mode.</p>
<p>The <strong>min Contour Size</strong> option can be used to dismiss all contours that have a smaller size than the given pixels.</p>
<p>By checking <strong>Show Skeleton</strong> the skeleton of each contour is calculated. The slider can be used to set contour smoothing before skeleton calculation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The skeleton is dynamically calculated. To avoid unecessary waiting, turn off skeleton calculation when working with many or complex contours or making changes to contours.</p>
</div>
<p>Press the <strong>Use as Stack label</strong> button to use the currently shown contours for all frames in the currently active stack. Only applicable when using a multi-frame image stack.</p>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading">#</a></h2>
<p>See <a class="reference internal" href="../results/index.html"><span class="doc">Results</span></a> for more information.</p>
<p>To open the result settings for segmentation press <a class="inline-icon reference internal" href="../_images/settings.png"><img alt="settings" class="inline-icon" src="../_images/settings.png" style="width: 16px; height: 16px;" /></a> <em>Settings</em> in the results window.</p>
<figure class="align-center" id="id30">
<img alt="../_images/segmentation_ressettings.png" class="shadow-image" src="../_images/segmentation_ressettings.png" />
<figcaption>
<p><span class="caption-text">Export options for segmentation</span><a class="headerlink" href="#id30" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>You can select the export options for segmentation. You have the option to export <strong>Skeleton</strong> size, the <strong>Perimeter</strong> for each contour, the <strong>Min Intensity</strong> minimum intensity value inside each contour, the <strong>Mean Intensity</strong> mean intensity value inside each contour and the <strong>Max Intensity</strong> maximum intensity value inside each contour.</p>
<hr class="docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="gc" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Rother, C., Kolmogorov, V. and Blake, A., 2004. “ GrabCut” interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3), pp.309-314.</p>
</aside>
<aside class="footnote brackets" id="hed" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Xie, S. and Tu, Z., 2015. Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision (pp. 1395-1403).</p>
</aside>
<aside class="footnote brackets" id="id25" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Maninis, K.K., Caelles, S., Pont-Tuset, J. and Van Gool, L., 2018. Deep extreme cut: From extreme points to object segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 616-625).</p>
</aside>
<aside class="footnote brackets" id="unet" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.</p>
</aside>
<aside class="footnote brackets" id="fcn" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Long, J., Shelhamer, E. and Darrell, T., 2015. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).</p>
</aside>
<aside class="footnote brackets" id="fpn" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">6</a><span class="fn-bracket">]</span></span>
<p>Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B. and Belongie, S., 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).</p>
</aside>
<aside class="footnote brackets" id="lnkn" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">7</a><span class="fn-bracket">]</span></span>
<p>Chaurasia, A. and Culurciello, E., 2017, December. Linknet: Exploiting encoder representations for efficient semantic segmentation. In 2017 IEEE Visual Communications and Image Processing (VCIP) (pp. 1-4). IEEE.</p>
</aside>
<aside class="footnote brackets" id="psp" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">8</a><span class="fn-bracket">]</span></span>
<p>Zhao, H., Shi, J., Qi, X., Wang, X. and Jia, J., 2017. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2881-2890).</p>
</aside>
<aside class="footnote brackets" id="dlv3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">9</a><span class="fn-bracket">]</span></span>
<p>Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F. and Adam, H., 2018. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV) (pp. 801-818).</p>
</aside>
<aside class="footnote brackets" id="imn" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">10</a><span class="fn-bracket">]</span></span>
<p>Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248-255). Ieee.</p>
</aside>
<aside class="footnote brackets" id="dns" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">11</a><span class="fn-bracket">]</span></span>
<p>Huang, G., Liu, Z., Van Der Maaten, L. and Weinberger, K.Q., 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).</p>
</aside>
<aside class="footnote brackets" id="eff" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">12</a><span class="fn-bracket">]</span></span>
<p>Tan, M. and Le, Q., 2019, May. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (pp. 6105-6114). PMLR.</p>
</aside>
<aside class="footnote brackets" id="inc" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">13</a><span class="fn-bracket">]</span></span>
<p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. and Wojna, Z., 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).</p>
</aside>
<aside class="footnote brackets" id="mob" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">14</a><span class="fn-bracket">]</span></span>
<p>Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M. and Adam, H., 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.</p>
</aside>
<aside class="footnote brackets" id="res" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">15</a><span class="fn-bracket">]</span></span>
<p>He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</p>
</aside>
<aside class="footnote brackets" id="rsx" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">16</a><span class="fn-bracket">]</span></span>
<p>Xie, S., Girshick, R., Dollár, P., Tu, Z. and He, K., 2017. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1492-1500).</p>
</aside>
<aside class="footnote brackets" id="sen" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id18">1</a>,<a role="doc-backlink" href="#id19">2</a>,<a role="doc-backlink" href="#id20">3</a>)</span>
<p>Hu, J., Shen, L. and Sun, G., 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).</p>
</aside>
<aside class="footnote brackets" id="vgg" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">18</a><span class="fn-bracket">]</span></span>
<p>Simonyan, K. and Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</p>
</aside>
<aside class="footnote brackets" id="xcp" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">19</a><span class="fn-bracket">]</span></span>
<p>Chollet, F., 2017. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1251-1258).</p>
</aside>
<aside class="footnote brackets" id="fl" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">20</a><span class="fn-bracket">]</span></span>
<p>Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).</p>
</aside>
<aside class="footnote brackets" id="vnet" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">21</a><span class="fn-bracket">]</span></span>
<p>Milletari, F., Navab, N. and Ahmadi, S.A., 2016, October. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV) (pp. 565-571). IEEE.</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="objectdetection.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Object Detection</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="classification.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Classification</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022, Nils Körber
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Segmentation</a><ul>
<li><a class="reference internal" href="#labelling">Labelling</a><ul>
<li><a class="reference internal" href="#tools">Tools</a></li>
<li><a class="reference internal" href="#automated-and-semi-automated-labelling">Automated and Semi-Automated Labelling</a><ul>
<li><a class="reference internal" href="#smart-mode">Smart Mode</a></li>
<li><a class="reference internal" href="#auto-segmentation">Auto-Segmentation</a></li>
<li><a class="reference internal" href="#dextr">DEXTR</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#segmentation-settings">Segmentation Settings</a></li>
<li><a class="reference internal" href="#neural-network-architectures">Neural Network architectures</a></li>
<li><a class="reference internal" href="#losses-and-metrics">Losses and Metrics</a><ul>
<li><a class="reference internal" href="#cross-entropy">Cross Entropy</a></li>
<li><a class="reference internal" href="#focal-loss">Focal Loss</a></li>
<li><a class="reference internal" href="#kullback-leibler-divergence">Kullback-Leibler Divergence</a></li>
<li><a class="reference internal" href="#dice-loss">Dice Loss</a></li>
<li><a class="reference internal" href="#intersection-over-union">Intersection over Union</a></li>
<li><a class="reference internal" href="#pixel-accuracy">Pixel Accuracy</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#postprocessing">Postprocessing</a></li>
<li><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>